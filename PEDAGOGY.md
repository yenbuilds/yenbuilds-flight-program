# Pedagogy Fundamentals

## 1. Learning Is a Process, Not an Event

**Core idea**  
Learning happens over time through repeated exposure, practice, feedback, and consolidation. Single events or one-off evaluations do not meaningfully change skill.

**Implication**  
- Treat each flight as a single data point in a longer trajectory  
- Emphasize trends over time, not isolated scores  
- Frame progress as directionality (improving / stable / degrading)

**Applied example**  
- Show rolling stability metrics across multiple approaches  
- Highlight “approach energy management improving over last 5 flights”

**Expert phrasing**  
“The system is designed around longitudinal skill acquisition rather than episodic performance.”

---

## 2. Feedback Must Be Immediate and Interpretable

**Core idea**  
Feedback only works when the learner can connect it directly to a recent action. Delayed or abstract feedback loses instructional value.

**Implication for your overlay**  
- Deliver feedback inside the flight phase where it occurs  
- Tie every alert to a concrete control input or decision  
- Avoid end-of-flight data dumps without causal explanation

**Applied example**  
Instead of “Unstable approach detected,” use:  
“Sink rate exceeded stable envelope below 500 ft due to delayed thrust response.”

---

## 3. Cognitive Load Is the Primary Constraint

**Core idea**  
Human working memory is limited. Too much information prevents learning even if the information is correct.

**Implication**  
- Show only phase-relevant information  
- Suppress non-actionable data during high-workload phases  
- Prefer symbolic or qualitative cues over raw numbers

**Applied example**  
- Minimalist approach-mode UI  
- Expanded analytics only after landing

**Expert phrasing**  
“We aggressively manage extraneous cognitive load during high-demand flight phases.”

---

## 4. Learners Build Mental Models, Not Checklists

**Core idea**  
True learning comes from understanding cause-and-effect relationships, not memorizing rules.

**Implication**  
- Expose causal chains instead of isolated metrics  
- Correlate pitch, power, configuration, and outcomes  
- Make instability explainable, not mysterious

**Applied example**  
“Late flap extension increased drag, which caused thrust lag and a sink rate spike.”

---

## 5. Mastery Requires Repetition With Variation

**Core idea**  
Repeating identical tasks builds fragile skill. Varying conditions builds robust competence.

**Implication**  
- Target performance across different conditions (wind, weight, runway)  
- Target consistency across variability, not perfection in one scenario

**Applied example**  
- Stability outcomes grouped by crosswind strength  
- Highlight “stable across diverse conditions”

**Expert phrasing**  
“The system prioritizes adaptive mastery over rote repetition.”

---

## 6. Assessment Should Be Diagnostic, Not Judgmental

**Core idea**  
Judgment shuts down learning, but diagnostics guide improvement.

**Implication**  
- AVOID pass/fail framing  
- Identify dominant contributing factors instead of issuing verdicts  
- Treat performance as a system state, not a personal flaw

**Applied example**  
“Energy management was the primary constraint on this approach.”

---

## 7. Motivation Depends on Learner Agency

**Core idea**  
Learners engage more deeply when they retain control over focus and goals.

**Implication**  
- Which metrics to prioritize? This should be a choice.  
- Have self-directed training modes  
- Do not force a single “correct” learning path

**Applied example**  
- User selects “focus on energy management this week”  
- Adapt feedback emphasis accordingly

**Expert phrasing**  
“The system preserves learner agency by making feedback navigable rather than prescriptive.”

---

## 8. Transfer Is the Only Meaningful Outcome

**Core idea**  
If learning does not transfer to new contexts, it is not real learning.

**Implication**  
- Use real-world aviation concepts and language  
- Emphasise principles over simulator-specific quirks  
- Make explicit links to operational heuristics

**Applied example**  
“This behaviour aligns with real-world stabilized approach criteria.”

---

## 9. Expertise Emerges Through Pattern Recognition

**Core idea**  
Experts do not calculate more—they recognize patterns faster and earlier.

**Implication for your overlay**  
- Cluster flights into recognizable instability archetypes  
- Use consistent labels for recurring patterns  
- Help users anticipate outcomes before they occur

**Applied example**  
- “High-energy late configuration”  
- “Drag-induced sink below gate”

**Expert phrasing**  
“The solution externalizes expert pattern recognition for developing pilots.”

---

## 10. Reflection Is Where Learning Consolidates

**Core idea**  
Experience alone does NOT produce learning. Reflection converts experience into insight.

**Implication**  
- Give minimal but intentional post-flight reflection  
- Focus questions on DECISIONS - NOT OUTCOMES
- Keep reflection lightweight but structured

**Applied example**  
“What would you change about power management below 1000 ft on this approach?”

**Expert phrasing**  
“The system scaffolds post-flight reflection to consolidate learning.”

---

## Strategic Positioning Summary

This system is:

**“A formative, feedback-driven learning system designed to accelerate pilot mental model formation under realistic cognitive constraints.”**

---
